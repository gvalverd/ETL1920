{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> $\\color{black}{\\text{TAREA FINAL: Datos de vivienda}}$</center></h1>\n",
    "\n",
    "---\n",
    "\n",
    "<h3><center> $\\color{black}{\\text{Beatriz Quevedo Gómez}}$  </center></h3> \n",
    "<h4><center> $\\color{black}{\\text{ETL, enero 2021}}$  </center></h4>\n",
    "\n",
    "\n",
    "<p> El negocio inmobiliario es uno de los motores de la economía de España, el carácter turístico de nuesta península hace que sean muchos los inversores extranjeros que se decidan por buscar una inversión de bajo riesgos en nuestras ciudades costeras o grandes urbes. \n",
    "Muchas de estas inversiones se hacen sobre grandes bolsas de inmuebles que deben ser analizados previamente para comprobar la rentabilidad del porfolio </p>\n",
    "\n",
    "<!-- <p> En este caso vamos a trabajar con una tabla que contienen información de distintos inmuebles repartidos por una zona específica, sus carácterísticas y su precio </p>  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todas las cuestiones se deben realizar sobre el conjunto de casos que representen viviendas ubicadas en zonas residenciales **(alta, media y baja densidad)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MUY IMPORTANTE:** En las otras prácticas he detectado colaboraciones involucrando varias personas y he sido flexible aunque a algunos os lo he mencionado en las correcciones, porque al final el trabajo de analista de datos es un trabajo colaborativo. Sin embargo, este trabajo es individual, así que cuidado con las colaboraciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las variables de las que se compone el dataset son:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|NOMBRE VARIABLE|DESCRIPTOR|VALORES|\n",
    "| --- | --- | --- |\n",
    "|Order|Variable de identificación|1 a 2930|\n",
    "|MS Zoning|Zona de ubicación de la vivienda|\"A rural, C comercial, FV residencial flotante, I industrial, RH residencial alta densidad, RL residencial baja densidad, RM residencial media densidad\"|\n",
    "|Lot Frontage|Longitud de la fachada en pies||\n",
    "|Lot Area|Superficie de la vivienda en pies cuadrados||\n",
    "|Land Contour|Contorno del terreno circundante|\"Lvl llano, Bnk Tipo bancal, HLS Ladera, Low Depresión\"|\n",
    "|Land Slope|Tipo de pendiente de la vivienda|\" Gtl pendiente suave, Mod pendiente moderada, Sev fuerte pendiente\"|\n",
    "|Overall Qual|Grado de calidad de materiales y acabado de la vivienda|De 1 (Muy pobre) a 10 (Excelente)|\n",
    "|Year Built|Año de construccion de la vivienda||\n",
    "|Year Remod/Add|Año de última reforma de la vivienda||\n",
    "|Mas Vnr Type|Tipo de revestimiento exterior|\" BrkCmn Ladrillo normal, BrkFace Ladrillo visto, CBlock Bloque de cemento, None Ninguna, Stone Piedra \"|\n",
    "|Exter Qual|Calidad de revestimiento exterior|\"Ex Excelente,Gd Bueno,TA Media,Fa Justo\"|\n",
    "|Bsmt Cond|Estado general del sótano|\"Ex Excelente, Gd Bueno, TA Media, Fa Justo, Po Pobre,Ss sin sótano\"|\n",
    "|Total Bsmt SF|Superficie del sótano en pies cuadrados|\n",
    "|Heating QC|Calidad de la calefacción|\"Ex Excelente,Gd Bueno,TA Media,Fa Justo,Po Pobre\"|\n",
    "|Central Air|Aire acondicionado centralizado|\"N No Y Sí\"|\n",
    "|Full Bath|Número de baños completo en planta||\n",
    "|Half Bath|Número de aseos en planta||\n",
    "|Bedroom AbvGr|Número de dormitorios en planta||\n",
    "|Kitchen AbvGr|Número de cocinas en planta||\n",
    "|Kitchen Qual|Calidad de cocinas|\"Ex Excelente,Gd Bueno,TA Media,Fa Justo,Po Pobre\"|\n",
    "|TotRms AbvGrd|Número total de habitaciones excluidos los cuartos de baño||\n",
    "|Garage Cars|Número de plazas de garaje||\n",
    "|Garage Area|Superficie del garaje|||\n",
    "|Garage Cond|Estado del garaje|\"Ex Excelente,Gd Bueno,TA Media,Fa Justo,Po Pobre,Sg sin garaje\"|\n",
    "|Pool Area|Superficie de la piscina en pies cuadrados|\n",
    "|Pool QC|Calidad de la piscina|\"Ex Excelente,Gd Bueno,TA Media,Fa Justo,Sp no hay piscina\"|\n",
    "|Mo Sold|mes de venta||\n",
    "|Yr Sold|año de venta||\n",
    "|SalePrice|precio de venta en dólares||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recomiendo al leer los datos, eliminar espacios de los nombres de las columnas, realiza un pequeño análisis inicial de los mismos. No olvides fijarte en los tipos de variables, que variables pueden tener tipos confundidos y corrige los. Sobre todo, trabaja con las fechas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTA:** Las tareas complementarias sirven para subir nota. El resto de preguntas valen igual y suman 10 puntos.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicializar y cargar el contexto spark\n",
    "\n",
    "En este primer apartado se realizará el parseado y tratamiento de los datos, así como la inicialización y carga del contexto Spark y de las librerías.\n",
    "\n",
    "Se importa `pyspark` (una vez realizada la instalación de Spark a través de la terminal) y se cargan las librerías de los módulos `pyspark.sql` (**Row** para las filas de datos de un DataFrame, **Column** para las columnas) y **SparkContext** y **SQLContext** (el punto de partida para trabajar con datos estructurados –filas y columnas– en Spark) así como `SparkContext` (representa la conexión con Spark cluster, que puede ser usado para crear RDD) de pyspark, que se definirá como sc y `SparkSession`, que sirve para crear datasets de un determinado RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import Column\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además, se cargan las líbrerías `pandas` y `numpy`, que se utilizarán a lo largo del trabajo, y `matplotlib` y `seaborn` para gráficos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un fichero `xlsx`, procedente de excel se puede leer de varias maneras en **Spark** utilizando el paquete `spark-excel` de *crealytics*, una librería para consultar archivos de Excel con Spark. \n",
    "\n",
    "Una opción para cargar un fichero de excel, es:\n",
    "\n",
    "`` spark = SparkSession.builder \\\n",
    ".master(\"local\") \\\n",
    ".appName(\"Word Count\") \\\n",
    ".config(\"spark.jars.packages\", \"com.crealytics:spark-excel_2.12:0.13.6\") \\\n",
    ".getOrCreate() ``\n",
    "\n",
    "Configurando primero la sesión en spark con el paquete, para después crear un DataFrame con los datos importados de excel.\n",
    "\n",
    "`` df = spark.read.format(\"com.crealytics.spark.excel\") \\\n",
    ".option(\"header\", \"true\") \\\n",
    ".option(\"inferSchema\", \"true\") \\\n",
    ".option(\"dataAddress\", \"'NameOfYourExcelSheet'!A1\") \\\n",
    ".load(\"./data/BDpracticafinal2021.xlsx\") ``\n",
    "\n",
    "Otra opción es definir una función que realice esto para cada archivo (es más práctico que la primera opción cuando se tienen varios ficheros de datos). \n",
    "\n",
    "Nótese que *file_name* corresponde a la ruta del archivo.\n",
    "\n",
    "`` def get_df_from_excel(SQLContext, file_name):\n",
    "    return SQLContext.read.format(\"com.crealytics.spark.excel\") \\\n",
    "        .option(\"useHeader\", \"true\") \\\n",
    "        .option(\"treatEmptyValuesAsNulls\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .option(\"addColorColumns\", \"False\") \\\n",
    "        .option(\"maxRowsInMey\", 2000) \\\n",
    "        .option(\"sheetName\", \"Import\") \\\n",
    "        .load(file_name)\n",
    "get_df_from_excel(SQLContext, './data/BDpracticafinal2021.xlsx') ``\n",
    "\n",
    "No obstante, debido a problemas con los complementos que son necesarios para leer excel con Spark, ambas opciones dan error, ya que **no es posible hacerlo con Spark básico**. Por tanto se leerá el fichero como `csv`. \n",
    "\n",
    "Se cargará el fichero de datos (en csv) como *raw data*, es decir, los datos en bruto sin procesar, con ayuda de la función `textFile()` usándola en `sc` con el archivo como argumento y creando un RDD (`rawData`).\n",
    "\n",
    "El archivo se encuentra en una carpeta llamada `data`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFile = './data/BDpracticafinalCSV.csv'\n",
    "rawData = sc.textFile(dataFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Order;MS Zoning;Lot Frontage;Lot Area;Land Contour;Land Slope;Overall Qual;Year Built;Year Remod/Add;Mas Vnr Type;Exter Qual;Bsmt Cond;Total Bsmt SF;Heating QC;Central Air;Full Bath;Half Bath;Bedroom AbvGr;Kitchen AbvGr;Kitchen Qual;TotRms AbvGrd;Garage Cars;Garage Area;Garage Cond;Pool Area;Pool QC;Mo Sold;Yr Sold;SalePrice',\n",
       " '1;RL;141;31770;Lvl;Gtl;6;1960;1960;Stone;TA;Gd;1080;Fa;Y;1;0;3;1;TA;7;2;528;TA;0;Sp;5;2010;215000']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawData.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se aprecia cómo el RDD tienen en la primera fila el nombre de cada variable (es decir, las columnas), lo cual es el encabezado. \n",
    "\n",
    "Para ello, se asignará esta fila a un objeto con `rawData.take(1)[0]` para posteriormente aplicar un filtro `rawData.filter(lambda x: x !=headRaw)` que elimine este objeto, siendo headRaw el nombre del objeto creado. \n",
    "\n",
    "No será necesario tratar el nombre de las columas eliminando los espacios porque en el presente trabajo se accederá a ellas mediante su posición, no su nombre, ya que se elimina el encabezado y resulta más eficiente. (De querer realizar este tratamiento, se debería usar una Regular Expression que reemplace los espacios por `_` o eliminarlos, por ejemplo)\n",
    "\n",
    "Simultáneamente, se realizará un **parseado** de datos a través de la función `.map()` separando por punto coma `;`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1',\n",
       "  'RL',\n",
       "  '141',\n",
       "  '31770',\n",
       "  'Lvl',\n",
       "  'Gtl',\n",
       "  '6',\n",
       "  '1960',\n",
       "  '1960',\n",
       "  'Stone',\n",
       "  'TA',\n",
       "  'Gd',\n",
       "  '1080',\n",
       "  'Fa',\n",
       "  'Y',\n",
       "  '1',\n",
       "  '0',\n",
       "  '3',\n",
       "  '1',\n",
       "  'TA',\n",
       "  '7',\n",
       "  '2',\n",
       "  '528',\n",
       "  'TA',\n",
       "  '0',\n",
       "  'Sp',\n",
       "  '5',\n",
       "  '2010',\n",
       "  '215000'],\n",
       " ['2',\n",
       "  'RH',\n",
       "  '80',\n",
       "  '11622',\n",
       "  'Lvl',\n",
       "  'Gtl',\n",
       "  '5',\n",
       "  '1961',\n",
       "  '1961',\n",
       "  'None',\n",
       "  'TA',\n",
       "  'TA',\n",
       "  '882',\n",
       "  'TA',\n",
       "  'Y',\n",
       "  '1',\n",
       "  '0',\n",
       "  '2',\n",
       "  '1',\n",
       "  'TA',\n",
       "  '5',\n",
       "  '1',\n",
       "  '730',\n",
       "  'TA',\n",
       "  '0',\n",
       "  'Sp',\n",
       "  '6',\n",
       "  '2010',\n",
       "  '105000']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headRaw = rawData.take(1)[0]\n",
    "filteredRaw = rawData.filter(lambda x: x !=headRaw)\n",
    "splitRaw = filteredRaw.map(lambda x: x.split(';'))\n",
    "splitRaw.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adicionalmente, como el enunciado dice que todas las cuestiones se deben realizar sobre el conjunto de casos que representen viviendas ubicadas en **zonas residenciales** (*alta, media y baja densidad*), se realizará un filtro para limitar el RDD a unicamente las filas que contengan estos valores. \n",
    "\n",
    "Además en este filtro se aplicará a la columna que establece las zonas una función que transforme los errores de codificación, ya que dentro de las zonas residenciales de baja densidad (`RL`) se encuentran valores codificados como `rL` y `Rl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zona_transf(x):\n",
    "    if x in \"Rl\":\n",
    "        return \"RL\"\n",
    "    elif x in \"rL\":\n",
    "        return \"RL\"\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1',\n",
       "  'RL',\n",
       "  '141',\n",
       "  '31770',\n",
       "  'Lvl',\n",
       "  'Gtl',\n",
       "  '6',\n",
       "  '1960',\n",
       "  '1960',\n",
       "  'Stone',\n",
       "  'TA',\n",
       "  'Gd',\n",
       "  '1080',\n",
       "  'Fa',\n",
       "  'Y',\n",
       "  '1',\n",
       "  '0',\n",
       "  '3',\n",
       "  '1',\n",
       "  'TA',\n",
       "  '7',\n",
       "  '2',\n",
       "  '528',\n",
       "  'TA',\n",
       "  '0',\n",
       "  'Sp',\n",
       "  '5',\n",
       "  '2010',\n",
       "  '215000']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = splitRaw.filter(lambda x: zona_transf(x[1]) == 'RL' or x[1] == 'RM' or x[1] == 'RH')\n",
    "data.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por lo tanto la longitud del RDD será de **2768 filas**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2768"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.map(lambda x: x[0]).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ver los tipos de cada columna se transforma el RDD en un dataFrame (se ejecuta antes la lína con el **contexto SQL** para que no de problemas), obteniendo el esquema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "SQLContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: string (nullable = true)\n",
      " |-- _4: string (nullable = true)\n",
      " |-- _5: string (nullable = true)\n",
      " |-- _6: string (nullable = true)\n",
      " |-- _7: string (nullable = true)\n",
      " |-- _8: string (nullable = true)\n",
      " |-- _9: string (nullable = true)\n",
      " |-- _10: string (nullable = true)\n",
      " |-- _11: string (nullable = true)\n",
      " |-- _12: string (nullable = true)\n",
      " |-- _13: string (nullable = true)\n",
      " |-- _14: string (nullable = true)\n",
      " |-- _15: string (nullable = true)\n",
      " |-- _16: string (nullable = true)\n",
      " |-- _17: string (nullable = true)\n",
      " |-- _18: string (nullable = true)\n",
      " |-- _19: string (nullable = true)\n",
      " |-- _20: string (nullable = true)\n",
      " |-- _21: string (nullable = true)\n",
      " |-- _22: string (nullable = true)\n",
      " |-- _23: string (nullable = true)\n",
      " |-- _24: string (nullable = true)\n",
      " |-- _25: string (nullable = true)\n",
      " |-- _26: string (nullable = true)\n",
      " |-- _27: string (nullable = true)\n",
      " |-- _28: string (nullable = true)\n",
      " |-- _29: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfData = SQLContext.createDataFrame(data)\n",
    "dfData.registerTempTable(\"dfData\")\n",
    "dfData.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede apreciar, todas las columas son de tipo **string**, por lo que a continuación se realizarán las transformaciones necesarias para que cada columna tenga su tipo correcto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_float(x):\n",
    "    if x == '':\n",
    "        xn = 0\n",
    "    else:\n",
    "        xn = float(x)\n",
    "    return(xn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(lambda x: [int(x[0]), x[1], type_float(x[2]), type_float(x[3]),\n",
    "                                x[4], x[5], type_float(x[6]), str(x[7]), \n",
    "                                type_float(x[8]), x[9], x[10], x[11], type_float(x[12]),\n",
    "                                x[13], x[14], type_float(x[15]), type_float(x[16]),\n",
    "                                type_float(x[17]), type_float(x[18]), x[19], \n",
    "                                type_float(x[20]), type_float(x[21]), type_float(x[22]),\n",
    "                                x[23], type_float(x[24]), x[25], type_float(x[26]), \n",
    "                                x[27], type_float(x[28])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: long (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: double (nullable = true)\n",
      " |-- _4: double (nullable = true)\n",
      " |-- _5: string (nullable = true)\n",
      " |-- _6: string (nullable = true)\n",
      " |-- _7: double (nullable = true)\n",
      " |-- _8: string (nullable = true)\n",
      " |-- _9: double (nullable = true)\n",
      " |-- _10: string (nullable = true)\n",
      " |-- _11: string (nullable = true)\n",
      " |-- _12: string (nullable = true)\n",
      " |-- _13: double (nullable = true)\n",
      " |-- _14: string (nullable = true)\n",
      " |-- _15: string (nullable = true)\n",
      " |-- _16: double (nullable = true)\n",
      " |-- _17: double (nullable = true)\n",
      " |-- _18: double (nullable = true)\n",
      " |-- _19: double (nullable = true)\n",
      " |-- _20: string (nullable = true)\n",
      " |-- _21: double (nullable = true)\n",
      " |-- _22: double (nullable = true)\n",
      " |-- _23: double (nullable = true)\n",
      " |-- _24: string (nullable = true)\n",
      " |-- _25: double (nullable = true)\n",
      " |-- _26: string (nullable = true)\n",
      " |-- _27: double (nullable = true)\n",
      " |-- _28: string (nullable = true)\n",
      " |-- _29: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfData = SQLContext.createDataFrame(data)\n",
    "dfData.registerTempTable(\"dfData\")\n",
    "dfData.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente se puede apreciar que los datos han sido transformados a su tipo correspondiente de manera satisfactoria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cuántas viviendas distintas encontramos en el dataset? ¿Se repite alguna? Tiene sentido que haya duplicadas? ¿Qué podemos hacer con las duplicadas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mediante una función `.count()` aplicada a la primera columna (*aunque da igual a cuál*) se observa que el dataset cuenta con datos de **2768 viviendas**, como se ha mencionado anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2768"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.map(lambda x: x[0]).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, de estas 2768 tan solo **hay 2762 viviendas distintas**, lo que significa que hay **6 filas repetidas**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2762"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.map(lambda x: x[0]).distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por tanto se **eliminarán los duplicados** mediante `dropDuplicates()`, función que se realiza sobre un dataFrame de los datos, ya que son filas que no proporcionan información nueva, meten ruido, y además en un dataset con 2768 filas, eliminar 6 no supone ningún problema a los análisis y ejercicios posteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "SQLContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2762"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfData = SQLContext.createDataFrame(data)\n",
    "dfData.registerTempTable(\"dfData\")\n",
    "dfData.dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se comprueba que se ha realizado bien el proceso de eliminar los valores duplicados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2762"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfClean = dfData.dropDuplicates().orderBy(dfData[0])\n",
    "dfClean.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y se realiza una tabla del dataset completo para asegurarse de que las filas están ordenadas en función de la primera columna, `Order`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+-------+---+---+---+----+------+-------+---+---+------+---+---+---+---+---+---+---+----+---+-----+---+---+---+---+----+--------+\n",
      "| _1| _2|   _3|     _4| _5| _6| _7|  _8|    _9|    _10|_11|_12|   _13|_14|_15|_16|_17|_18|_19|_20| _21|_22|  _23|_24|_25|_26|_27| _28|     _29|\n",
      "+---+---+-----+-------+---+---+---+----+------+-------+---+---+------+---+---+---+---+---+---+---+----+---+-----+---+---+---+---+----+--------+\n",
      "|  1| RL|141.0|31770.0|Lvl|Gtl|6.0|1960|1960.0|  Stone| TA| Gd|1080.0| Fa|  Y|1.0|0.0|3.0|1.0| TA| 7.0|2.0|528.0| TA|0.0| Sp|5.0|2010|215000.0|\n",
      "|  2| RH| 80.0|11622.0|Lvl|Gtl|5.0|1961|1961.0|   None| TA| TA| 882.0| TA|  Y|1.0|0.0|2.0|1.0| TA| 5.0|1.0|730.0| TA|0.0| Sp|6.0|2010|105000.0|\n",
      "|  3| RL| 81.0|14267.0|Lvl|Gtl|6.0|1958|1958.0|BrkFace| TA| TA|1329.0| TA|  Y|1.0|1.0|3.0|1.0| Gd| 6.0|1.0|312.0| TA|0.0| Sp|6.0|2010|172000.0|\n",
      "|  4| RL| 93.0|11160.0|Lvl|Gtl|7.0|1968|1968.0|   None| Gd| TA|2110.0| Ex|  Y|2.0|1.0|3.0|1.0| Ex| 8.0|2.0|522.0| TA|0.0| Sp|4.0|2010|244000.0|\n",
      "|  5| RL| 74.0|13830.0|Lvl|Gtl|5.0|1997|1998.0|   None| TA| TA| 928.0| Gd|  Y|2.0|1.0|3.0|1.0| TA| 6.0|2.0|482.0| TA|0.0| Sp|3.0|2010|189900.0|\n",
      "|  6| RL| 78.0| 9978.0|Lvl|Gtl|6.0|1998|1998.0|BrkFace| TA| TA| 926.0| Ex|  Y|2.0|1.0|3.0|1.0| Gd| 7.0|2.0|470.0| TA|0.0| Sp|6.0|2010|195500.0|\n",
      "|  7| RL| 41.0| 4920.0|Lvl|Gtl|8.0|2001|2001.0|   None| Gd| TA|1338.0| Ex|  Y|2.0|0.0|2.0|1.0| Gd| 6.0|2.0|582.0| TA|0.0| Sp|4.0|2010|213500.0|\n",
      "|  8| RL| 43.0| 5005.0|HLS|Gtl|8.0|1992|1992.0|   None| Gd| TA|1280.0| Ex|  Y|2.0|0.0|2.0|1.0| Gd| 5.0|2.0|506.0| TA|0.0| Sp|1.0|2010|191500.0|\n",
      "|  9| RL| 39.0| 5389.0|Lvl|Gtl|8.0|1995|1996.0|   None| Gd| TA|1595.0| Ex|  Y|2.0|0.0|2.0|1.0| Gd| 5.0|2.0|608.0| TA|0.0| Sp|3.0|2010|236500.0|\n",
      "| 10| RL| 60.0| 7500.0|Lvl|Gtl|7.0|1999|1999.0|   None| TA| TA| 994.0| Gd|  Y|2.0|1.0|3.0|1.0| Gd| 7.0|2.0|442.0| TA|0.0| Sp|6.0|2010|189000.0|\n",
      "| 11| RL| 75.0|10000.0|Lvl|Gtl|6.0|1993|1994.0|   None| TA| TA| 763.0| Gd|  Y|2.0|1.0|3.0|1.0| TA| 7.0|2.0|440.0| TA|0.0| Sp|4.0|2010|175900.0|\n",
      "| 12| RL| null| 7980.0|Lvl|Gtl|6.0|1992|2007.0|   None| TA| TA|1168.0| Ex|  Y|2.0|0.0|3.0|1.0| TA| 6.0|2.0|420.0| TA|0.0| Sp|3.0|2010|185000.0|\n",
      "| 13| RL| 63.0| 8402.0|Lvl|Gtl|6.0|1998|1998.0|   None| TA| TA| 789.0| Gd|  Y|2.0|1.0|3.0|1.0| TA| 7.0|2.0|393.0| TA|0.0| Sp|5.0|2010|180400.0|\n",
      "| 14| RL| 85.0|10176.0|Lvl|Gtl|7.0|1990|1990.0|   None| TA| TA|1300.0| Gd|  Y|1.0|1.0|2.0|1.0| Gd| 5.0|2.0|506.0| TA|0.0| Sp|2.0|2010|171500.0|\n",
      "| 15| RL| null| 6820.0|Lvl|Gtl|8.0|1985|1985.0|   None| Gd| TA|1488.0| TA|  Y|1.0|1.0|1.0|1.0| Gd| 4.0|2.0|528.0| TA|0.0| Sp|6.0|2010|212000.0|\n",
      "| 16| RL| 47.0|53504.0|HLS|Mod|8.0|2003|2003.0|BrkFace| Ex| TA|1650.0| Ex|  Y|3.0|1.0|4.0|1.0| Ex|12.0|3.0|841.0| TA|0.0| Sp|6.0|2010|538000.0|\n",
      "| 17| RL|152.0|12134.0|Bnk|Mod|8.0|1988|2005.0|   None| Gd| TA| 559.0| Gd|  Y|2.0|0.0|4.0|1.0| TA| 8.0|2.0|492.0| TA|0.0| Sp|6.0|2010|164000.0|\n",
      "| 18| RL| 88.0|11394.0|Lvl|Gtl|9.0|2010|2010.0|  Stone| Gd| TA|1856.0| Ex|  Y|1.0|1.0|1.0|1.0| Ex| 8.0|3.0|834.0| TA|0.0| Sp|6.0|2010|394432.0|\n",
      "| 19| RL|140.0|19138.0|Lvl|Gtl|4.0|1951|1951.0|   None| TA| TA| 864.0| Ex|  Y|1.0|0.0|2.0|1.0| TA| 4.0|2.0|400.0| TA|0.0| Sp|6.0|2010|141000.0|\n",
      "| 20| RL| 85.0|13175.0|Lvl|Gtl|6.0|1978|1988.0|  Stone| TA| TA|1542.0| TA|  Y|2.0|0.0|3.0|1.0| TA| 7.0|2.0|500.0| TA|0.0| Sp|2.0|2010|210000.0|\n",
      "+---+---+-----+-------+---+---+---+----+------+-------+---+---+------+---+---+---+---+---+---+---+----+---+-----+---+---+---+---+----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfClean.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente este dataFrame se convierte en un RDD que será el RDD base de los ejercicios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataClean = dfClean.rdd.map(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ¿Podrías decirme el total de inmuebles y el precio medio (Sale Price) de cada zona (MS Zoning)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se ha observado anteriormente, hay tres zonas, RL, RH y RM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RL', 'RH', 'RM']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataClean.map(lambda x: zona_transf(x[1])).distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para saber el **precio medio** de la `zona RH (alta densidad)` se aplica un filtro limitando los registros a estas zonas. Se obtiene que el precio medio es de **136419.78**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136419.77777777775"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RH = dataClean.filter(lambda x: x[1] == 'RH')\n",
    "RH.map(lambda x: x[28]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y contando cuántas viviendas pertecen a esta zona mediante `count()`, se observa que son **27** los **inmuebles** de la zona de alta densidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RH.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se realiza el mismo proceso con la `zona de densidad baja`, concluyendo que el precio medio es de **191283.25** y es la zona con más inmuebles en el dataset (**2273**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191283.25164980206"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RL = dataClean.filter(lambda x: zona_transf(x[1]) == 'RL')\n",
    "RL.map(lambda x: x[28]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2273"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RL.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, en la `zona de densidad media` hay **462** inmuebles con un precio medio de **126781.39**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126781.39393939397"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RM = dataClean.filter(lambda x: x[1] == 'RM')\n",
    "RM.map(lambda x: x[28]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "462"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RM.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se suma el conteo de cada zona para asegurarse de que se ha realizado bien el ejercicio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2762"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RH.count()+RL.count()+RM.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Media de Total Bsmt SF por cada década de construcción calculada a partir de Year Built. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para realizar el ejercicio primero se observa de qué a qué década se comprenden los datos correspondientes al año de construcción de las viviendas (`Year Built`): de la década de 1870s hasta 2010. En total, 14 décadas diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1872'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataClean.map(lambda x: x[7]).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2010'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataClean.map(lambda x: x[7]).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para separar las décadas del RDD se realiza una función que se queda con los tres primeros valores del año y le añade un \"0s\". Es por esto por lo que al comienzo del trabajo, en el primer aparado del parseado, se codifica esta columna como cadena aunque sea una fecha, ya que de esta forma no dará problemas la aplicación de la función. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decadas(x):\n",
    "    return x[0:3] + '0s';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero se forma el RDD con el que se trabajará en este ejercicio: compuesto por dos columnas, las décadas (ya transformadas por la función) y la superficie del sótano en pies cuadrados (`Total Bsmt SF`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1870s', 780.0],\n",
       " ['1870s', 684.0],\n",
       " ['1870s', 819.0],\n",
       " ['1880s', 777.0],\n",
       " ['1880s', 1240.0]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec = dataClean.sortBy(lambda x: decadas(x[7])).map(lambda x: [decadas(x[7]), x[12]])\n",
    "dec.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posteriormente, a este RDD se le aplica un `combineByKey`, que hace la media la superficie del sótano (en pies cuadrados) y ordena los datos de mayor a menor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2010s', 1539.0),\n",
       " ('2000s', 1348.47),\n",
       " ('1990s', 1138.81),\n",
       " ('1980s', 1086.0),\n",
       " ('1960s', 1082.67),\n",
       " ('1950s', 968.3),\n",
       " ('1970s', 952.72),\n",
       " ('1890s', 885.58),\n",
       " ('1880s', 836.0),\n",
       " ('1920s', 833.94),\n",
       " ('1910s', 788.3),\n",
       " ('1930s', 769.92),\n",
       " ('1870s', 761.0),\n",
       " ('1900s', 720.14),\n",
       " ('1940s', 706.93)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg = dec.combineByKey(lambda value: (value, 1),\n",
    "                           lambda x, value: (x[0] + value, x[1] + 1),\n",
    "                           lambda x, y: (x[0] + y[0], x[1] + y[1])\n",
    "                           )\n",
    "\n",
    "avg = avg.map(lambda x: (x[0], round(x[1][0]/x[1][1],2)))\n",
    "avg.sortBy(lambda x: x[1], ascending = False).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* De esta manera se puede observar que la década con **más pies cuadrados en los sótanos es la de 2010**, mientras que la que **menos** es la década de los **40s**. \n",
    "\n",
    "    Además, parece que en las últimas cuatro décadas **ha ido aumentando** la superficie de los sótanos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Cuál es la decada de construcción con viviendas mejor acondicionadas para el frío (Heating QC)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la segunda parte del cuarto aparado se pide comparar la calidad de la calefacción con las décadas de construccion. Para ello se utilizará la agrupación realizada en el apartado anterior.\n",
    "\n",
    "Se observan cuáles son los valores que puede tomar la calidad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TA', 'Fa', 'Gd', 'Po', 'Ex']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataClean.map(lambda x: x[13]).distinct().take(9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero se crea el RDD que junte las décadas de cada vivienda junto con su estátus de calidad.\n",
    "\n",
    "TA corresponde a calidad **media**, Fa a **justa**, Gd a **buena**, Po a **pobre** y Ex a **excelente**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "heatingQC = dataClean.sortBy(lambda x: decadas(x[7])).map(lambda x: [decadas(x[7]), x[13]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se aplica `.reduceByKey()`, que opera sobre pares de clave-valor (x, y en este caso) y fusiona los valores de cada clave. Por lo tanto en este caso funciona como un .count() de cuántos valores de calidad (independientemente del nivel) hay en cada década. Esto es equivalente al número de viviendas que pertenecen a cada década. \n",
    "\n",
    "Además, mediante `.sortBy()` se ordena de mayor a menor número de viviendas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2000s', 663),\n",
       " ('1970s', 364),\n",
       " ('1960s', 357),\n",
       " ('1950s', 335),\n",
       " ('1990s', 313),\n",
       " ('1920s', 190),\n",
       " ('1940s', 149),\n",
       " ('1980s', 120),\n",
       " ('1930s', 107),\n",
       " ('1910s', 103),\n",
       " ('1900s', 36),\n",
       " ('1890s', 12),\n",
       " ('1880s', 8),\n",
       " ('1870s', 3),\n",
       " ('2010s', 2)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countFrio = heatingQC.map(lambda x: (x[0],1)).reduceByKey(lambda x, y: x + y).sortBy(lambda x: x[1], False)\n",
    "countFrio.collect()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así se puede ver cómo los años 80 y 70 del siglo XIX tienen muy pocas observaciones en el dataset, como ocurre con 2010.\n",
    "\n",
    "Esto se compara con un segundo RDD que también realiza un `.reduceByKey()` y un `.sortBy()`, pero restringiendo con un filtro los valores de calidad **Excelente** o **Buena**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2000s', 663),\n",
       " ('1990s', 305),\n",
       " ('1950s', 173),\n",
       " ('1960s', 167),\n",
       " ('1970s', 109),\n",
       " ('1920s', 94),\n",
       " ('1940s', 82),\n",
       " ('1910s', 66),\n",
       " ('1930s', 62),\n",
       " ('1980s', 62),\n",
       " ('1900s', 23),\n",
       " ('1890s', 8),\n",
       " ('1880s', 5),\n",
       " ('2010s', 2),\n",
       " ('1870s', 1)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mejorFrio = heatingQC.filter(lambda x: x[1] == 'Ex' or x[1] == 'Gd').map(lambda x: (x[0],1)).reduceByKey(lambda x, y: x + y).sortBy(lambda x: x[1], False)\n",
    "mejorFrio.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Por tanto, comparando ambos RDDs (`countFrio` y `mejorFrio`) se aprecia que la década de los 2000 tiene TODAS las viviendas calificadas con un bueno o excelente en la acondicionación del frio, lo que no sucede en esta década. Se concluye por tando que **la década de los 2000 es la que está mejor acondicionada**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ¿Cuáles son las 10 viviendas que se vendieron por un precio más elevado por metro cuadrado en el año 2009?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para responder a esta preguna, primero es fundamental realizar el **cambio de unidades** correctamente, ya que los datos están en pies cuadrados y en el ejercicio se piden metros cuadrádos. 1 pie cuadrado equivale a **0.092903 metros cuadrados**.\n",
    "\n",
    "Así, se crea una función `viviendas` con dos parámetros (precio y área) que calcula el **precio por metro cuadrado**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viviendas(price, area):\n",
    "    t = price / (area*0.092903)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mediante un filtro de **limitan las observaciones a aquellas de 2009**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "totViviendas = dataClean.filter(lambda x: x[27] == '2009')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[936, 1026.7118971897073],\n",
       " [934, 991.0325013521444],\n",
       " [464, 821.0570122815511],\n",
       " [935, 807.9613655043873],\n",
       " [408, 765.647528938273],\n",
       " [407, 756.0368904997173],\n",
       " [405, 717.5943367454944],\n",
       " [933, 714.281725843034],\n",
       " [403, 711.1872444531239],\n",
       " [411, 701.0707829388548]]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totViviendas.sortBy(lambda x: viviendas(x[28], x[3]), ascending = False).map(lambda x: [x[0], viviendas(x[28], x[3])]).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Después de ordenar por el precio por metro cuadrado (de mayor a menor) se obtienen las **diez viviendas con mayor precio/metro cuadrado en 2009**, siendo la más elevada la observación número 936, con 1026,71um/metro cuadrado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Media anual por zonas del precio de venta y metros cuadrados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este ejercicio se realizará en dos partes: primero se calculará la media anual por zonas de los metros cuadrados y posteriormente por el precio de venta. Ambas se realizarán de igual manera: \n",
    "* Primero se realiza un filtro indicando la zona correspondiente, así como las columnas del rdd que se está creando. Una de ellas será el año y la otra será o los metros cuadrados o el precio de venta.\n",
    "* Después se realizará un `.combineByKey()` junto a una función `.map()` que obtendrá la media de la variable que se esté calculando, agrupada por años. \n",
    "\n",
    "Nótese que en este ejercicio ha de hacerse de nuevo la conversión de pies a metros cuadrados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m2(area):\n",
    "    t = area*0.092903\n",
    "    return float(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`METROS CUADRADOS`\n",
    "* Zona RH (alta densidad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2007', 13679.97),\n",
       " ('2006', 13035.84),\n",
       " ('2008', 13966.42),\n",
       " ('2009', 10795.96),\n",
       " ('2010', 11731.79)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zonaRH = dataClean.filter(lambda x: x[1] == 'RH').sortBy(lambda x: x[27]).map(lambda x: [x[27], m2(x[28])])\n",
    "avg = zonaRH.combineByKey(lambda value: (value, 1),\n",
    "                           lambda x, value: (x[0] + value, x[1] + 1),\n",
    "                           lambda x, y: (x[0] + y[0], x[1] + y[1])\n",
    "                           )\n",
    "\n",
    "avg = avg.map(lambda x: (x[0], round(x[1][0]/x[1][1],2)))\n",
    "avg.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Zona RM (densidad media)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2007', 625.45),\n",
       " ('2006', 575.01),\n",
       " ('2008', 623.67),\n",
       " ('2009', 552.56),\n",
       " ('2010', 586.1)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zonaRM = dataClean.filter(lambda x: x[1] == 'RM').sortBy(lambda x: x[27]).map(lambda x: [x[27], m2(x[3])])\n",
    "avg = zonaRM.combineByKey(lambda value: (value, 1),\n",
    "                            lambda x, value: (x[0] + value, x[1] + 1),\n",
    "                            lambda x, y: (x[0] + y[0], x[1] + y[1])\n",
    "                           )\n",
    "\n",
    "avg = avg.map(lambda x: (x[0], round(x[1][0]/x[1][1],2)))\n",
    "avg.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Zona RL (densidad baja)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2007', 1059.68),\n",
       " ('2006', 1043.79),\n",
       " ('2008', 1047.65),\n",
       " ('2009', 1002.18),\n",
       " ('2010', 1011.36)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zonaRL = dataClean.filter(lambda x: zona_transf(x[1]) == 'RL').sortBy(lambda x: x[27]).map(lambda x: [x[27], m2(x[3])])\n",
    "avg = zonaRL.combineByKey(lambda value: (value, 1),\n",
    "                            lambda x, value: (x[0] + value, x[1] + 1),\n",
    "                            lambda x, y: (x[0] + y[0], x[1] + y[1])\n",
    "                           )\n",
    "\n",
    "avg = avg.map(lambda x: (x[0], round(x[1][0]/x[1][1],2)))\n",
    "avg.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PRECIO DE VENTA`\n",
    "* Zona RH (alta densidad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2007', 147250.0),\n",
       " ('2006', 140316.67),\n",
       " ('2008', 150333.33),\n",
       " ('2009', 116206.8),\n",
       " ('2010', 126280.0)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zonaRH = dataClean.filter(lambda x: x[1] == 'RH').sortBy(lambda x: x[27]).map(lambda x: [x[27], x[28]])\n",
    "avg = zonaRH.combineByKey(lambda value: (value, 1),\n",
    "                            lambda x, value: (x[0] + value, x[1] + 1),\n",
    "                            lambda x, y: (x[0] + y[0], x[1] + y[1])\n",
    "                           )\n",
    "\n",
    "avg = avg.map(lambda x: (x[0], round(x[1][0]/x[1][1],2)))\n",
    "avg.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Zona RM (densidad media)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2007', 128947.62),\n",
       " ('2006', 127502.64),\n",
       " ('2008', 133336.12),\n",
       " ('2009', 118759.87),\n",
       " ('2010', 119681.71)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zonaRM = dataClean.filter(lambda x: x[1] == 'RM').sortBy(lambda x: x[27]).map(lambda x: [x[27], x[28]])\n",
    "avg = zonaRM.combineByKey(lambda value: (value, 1),\n",
    "                            lambda x, value: (x[0] + value, x[1] + 1),\n",
    "                            lambda x, y: (x[0] + y[0], x[1] + y[1])\n",
    "                           )\n",
    "\n",
    "avg = avg.map(lambda x: (x[0], round(x[1][0]/x[1][1],2)))\n",
    "avg.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Zona RL (densidad baja)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2007', 193399.98),\n",
       " ('2006', 194350.64),\n",
       " ('2008', 189885.59),\n",
       " ('2009', 190455.55),\n",
       " ('2010', 184978.42)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zonaRL = dataClean.filter(lambda x: zona_transf(x[1]) == 'RL').sortBy(lambda x: x[27]).map(lambda x: [x[27], x[28]])\n",
    "avg = zonaRL.combineByKey(lambda value: (value, 1),\n",
    "                            lambda x, value: (x[0] + value, x[1] + 1),\n",
    "                            lambda x, y: (x[0] + y[0], x[1] + y[1])\n",
    "                           )\n",
    "\n",
    "avg = avg.map(lambda x: (x[0], round(x[1][0]/x[1][1],2)))\n",
    "avg.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ¿Podrías decirme el total de recaudación de las casas de revistimiento (Mas Vnr Type) de piedra con respecto a las de ladrillo? ¿Hay diferencia significativa?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El revestimiento de las casas puede tener las siguientes categorías: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', 'Stone', 'BrkFace', 'CBlock', 'BrkCmn', 'None']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataClean.map(lambda x: x[9]).distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso de este ejercicio se utilizarán unicamente `Stone` (piedra) y las dos de ladrillo: `BrkFace` y `BrkCmn`.\n",
    "\n",
    "Para saber el total de la recausdación (la suma de los precios de venta) de cada revestimiento, basta con filtrar aquel que se desea comprobar y luego sumar todos los valores de `Sale Price`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60175477.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalPiedra = dataClean.filter(lambda x: x[9] == 'Stone')\n",
    "totalPiedra.map(lambda x: x[28]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182417536.0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalLadrillo = dataClean.filter(lambda x: x[9] == 'BrkCmn' or x[9] == 'BrkFace')\n",
    "totalLadrillo.map(lambda x: x[28]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Como se puede comprobar, las **casas de ladrillo tienen una mayor recaudación** que las de piedra, de hecho, **3 veces más**, y diferencia de los precios de venta alcanza las 122,242,059um. \n",
    "\n",
    "Ahora bien, ¿es esta diferencia significativa?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.031426506182909"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalLadrillo.map(lambda x: x[28]).sum() / totalPiedra.map(lambda x: x[28]).sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122242059.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalLadrillo.map(lambda x: x[28]).sum() - totalPiedra.map(lambda x: x[28]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si se cuenta el número de viviendas de piedra y se compara con las de ladrillo, se puede observar cómo hay 3.6 veces más casas del último tipo. Lo cual justifica que su recaudación sea también 3 veces mayor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "229"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataClean.filter(lambda x: x[9] == 'Stone').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "873"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataClean.filter(lambda x: x[9] == 'BrkCmn' or x[9] == 'BrkFace').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.6345381526104417"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "905/249"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262775.0087336243"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalPiedra.map(lambda x: x[28]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208954.79495990832"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalLadrillo.map(lambda x: x[28]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adicionalmente, si en vez de realizar la recaudación total se calcula la recaudación media, se puede observar cómo las casas de piedra, de media, son más caras que las de ladrillo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ¿Cuánto son más caras las viviendas con 2 cocinas, con 2 o más plazas de garaje que las que tienen 1 cocina y 1 plaza de garaje? Comparar medias y cuartiles de ambos casos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para contestar a esta pregunta, primero se comprueba el número de cocinas y de plazas de garaje que pueden tener las viviendas del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 2.0, 3.0]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataClean.map(lambda x: x[18]).distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, None, 1.0, 2.0, 3.0, 4.0, 5.0]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataClean.map(lambda x: x[21]).distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se realizan dos grupos, para poder comparar:\n",
    "* **Grupo 1**: viviendas con 2 cocinas y con 2 o más plazas\n",
    "* **Grupo 2**: viviendas con 1 cocina y 1 plaza de garaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145124.2790697675"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Grupo1 = dataClean.filter(lambda x: x[18] == 2 and x[21] >= 2)\n",
    "Grupo1.map(lambda x: x[28]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128121.99337748351"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Grupo2 = dataClean.filter(lambda x: x[18] == 1 and x[21] == 1)\n",
    "Grupo2.map(lambda x: x[28]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta manera se puede observar que la media de precios del **primer grupo es superior al segundo**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "SQLContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[118964.0, 141000.0, 159000.0]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g1 = SQLContext.createDataFrame(Grupo1)\n",
    "g1.registerTempTable(\"g1\")\n",
    "g1.approxQuantile(\"_29\",[0.25, 0.5, 0.75], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pandas1 = g1.toPandas()\n",
    "pandas1.boxplot('_29')\n",
    "display(plt.show());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[111500.0, 128000.0, 142125.0]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g2 = SQLContext.createDataFrame(Grupo2)\n",
    "g2.registerTempTable(\"g2\")\n",
    "g2.approxQuantile(\"_29\",[0.25, 0.5, 0.75], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pandas2 = g2.toPandas()\n",
    "pandas2.boxplot('_29')\n",
    "display(plt.show());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si comparamos los cuartiles de ambos grupos, podemos apreciar que el segundo tiene **menos varianza**, ya que su primer cuartil se encuentra en las 111,500um y su tercero en las 142,125um, mientras que el primer grupo 118964um y 159000um. Sin embargo, el segundo grupo tiene **más outliers** que el primero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (COMPLEMENTARIA) Estudiar la relación entre el precio y el número de Garajes. (Recomiendo segmentar precio por cuartiles y estudiar el número de casos coincidentes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero se crea un RDD que contenga las dos columnas que se utilizarán en este ejercicio: precio y número de garajes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[215000.0, 2.0],\n",
       " [105000.0, 1.0],\n",
       " [172000.0, 1.0],\n",
       " [244000.0, 2.0],\n",
       " [189900.0, 2.0]]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel = dataClean.map(lambda x: [x[28], x[21]])\n",
    "rel.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "SQLContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posteriormente se crea un dataFrame de este RDD para poder calcular los cuartiles de la variable precio, para poder agruparla en función de estos con una función."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[129000.0, 159000.0, 211500.0]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relDF = SQLContext.createDataFrame(rel)\n",
    "relDF.registerTempTable(\"relDF\")\n",
    "relDF.approxQuantile(\"_1\",[0.25, 0.5, 0.75], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cuartiles(x):\n",
    "    if 129000 > x :\n",
    "        return 'MENOS DE 129.000'\n",
    "    elif 129000 <= x < 159000 :\n",
    "        return 'ENTRE 129.000 Y 159.000'\n",
    "    elif 159000 <= x < 211500 :\n",
    "        return 'ENTRE 159.000 Y 211.500'\n",
    "    elif 211500 <= x :\n",
    "        return 'MAS DE 211.500'\n",
    "    else: \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuart = dataClean.sortBy(lambda x: cuartiles(x[28])).map(lambda x: [cuartiles(x[28]), x[21]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ENTRE 129.000 Y 159.000', 2.0],\n",
       " ['ENTRE 129.000 Y 159.000', 2.0],\n",
       " ['ENTRE 129.000 Y 159.000', 2.0],\n",
       " ['ENTRE 129.000 Y 159.000', 1.0],\n",
       " ['ENTRE 129.000 Y 159.000', 2.0]]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuart.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 334.0 failed 1 times, most recent failure: Lost task 0.0 in stage 334.0 (TID 20130, 192.168.1.43, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/beaquevedo/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/Users/beaquevedo/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 595, in process\n    out_iter = func(split_index, iterator)\n  File \"/Users/beaquevedo/opt/anaconda3/lib/python3.8/site-packages/pyspark/rdd.py\", line 2596, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/beaquevedo/opt/anaconda3/lib/python3.8/site-packages/pyspark/rdd.py\", line 2596, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/beaquevedo/opt/anaconda3/lib/python3.8/site-packages/pyspark/rdd.py\", line 425, in func\n    return f(iterator)\n  File \"/Users/beaquevedo/opt/anaconda3/lib/python3.8/site-packages/pyspark/rdd.py\", line 1946, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/Users/beaquevedo/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\n  File \"/Users/beaquevedo/opt/anaconda3/lib/python3.8/site-packages/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-76-71e8f53b5434>\", line 2, in <lambda>\nTypeError: unsupported operand type(s) for +: 'float' and 'NoneType'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2139)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:154)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor107.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/beaquevedo/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/Users/beaquevedo/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 595, in process\n    out_iter = func(split_index, iterator)\n  File \"/Users/beaquevedo/opt/anaconda3/lib/python3.8/site-packages/pyspark/rdd.py\", line 2596, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/beaquevedo/opt/anaconda3/lib/python3.8/site-packages/pyspark/rdd.py\", line 2596, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/beaquevedo/opt/anaconda3/lib/python3.8/site-packages/pyspark/rdd.py\", line 425, in func\n    return f(iterator)\n  File \"/Users/beaquevedo/opt/anaconda3/lib/python3.8/site-packages/pyspark/rdd.py\", line 1946, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/Users/beaquevedo/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\n  File \"/Users/beaquevedo/opt/anaconda3/lib/python3.8/site-packages/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-76-71e8f53b5434>\", line 2, in <lambda>\nTypeError: unsupported operand type(s) for +: 'float' and 'NoneType'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-71e8f53b5434>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                            )\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mavg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1446\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1116\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1118\u001b[0;31m         \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1119\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 334.0 failed 1 times, most recent failure: Lost task 0.0 in stage 334.0 (TID 20130, 192.168.1.43, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/beaquevedo/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/Users/beaquevedo/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 595, in process\n    out_iter = func(split_index, iterator)\n  File \"/Users/beaquevedo/opt/anaconda3/lib/python3.8/site-packages/pyspark/rdd.py\", line 2596, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/beaquevedo/opt/anaconda3/lib/python3.8/site-packages/pyspark/rdd.py\", line 2596, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/beaquevedo/opt/anaconda3/lib/python3.8/site-packages/pyspark/rdd.py\", line 425, in func\n    return f(iterator)\n  File \"/Users/beaquevedo/opt/anaconda3/lib/python3.8/site-packages/pyspark/rdd.py\", line 1946, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/Users/beaquevedo/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\n  File \"/Users/beaquevedo/opt/anaconda3/lib/python3.8/site-packages/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-76-71e8f53b5434>\", line 2, in <lambda>\nTypeError: unsupported operand type(s) for +: 'float' and 'NoneType'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2139)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:154)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor107.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/beaquevedo/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/Users/beaquevedo/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 595, in process\n    out_iter = func(split_index, iterator)\n  File \"/Users/beaquevedo/opt/anaconda3/lib/python3.8/site-packages/pyspark/rdd.py\", line 2596, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/beaquevedo/opt/anaconda3/lib/python3.8/site-packages/pyspark/rdd.py\", line 2596, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/Users/beaquevedo/opt/anaconda3/lib/python3.8/site-packages/pyspark/rdd.py\", line 425, in func\n    return f(iterator)\n  File \"/Users/beaquevedo/opt/anaconda3/lib/python3.8/site-packages/pyspark/rdd.py\", line 1946, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/Users/beaquevedo/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\n  File \"/Users/beaquevedo/opt/anaconda3/lib/python3.8/site-packages/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-76-71e8f53b5434>\", line 2, in <lambda>\nTypeError: unsupported operand type(s) for +: 'float' and 'NoneType'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "avg = cuart.combineByKey(lambda value: (value, 1),\n",
    "                           lambda x, value: (x[0] + value, x[1] + 1),\n",
    "                           lambda x, y: (x[0] + y[0], x[1] + y[1])\n",
    "                           )\n",
    "\n",
    "avg = avg.map(lambda x: (x[0], x[1][0]/x[1][1],2))\n",
    "avg.sortBy(lambda x: x[0], ascending = False).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (COMPLEMENTARIA) Las 10 viviendas con mejores servicios y mejor precio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pista:\n",
    "#### Calcula las variables: \n",
    "- Número de servicios excelentes\n",
    "- Número de servicios buenos\n",
    "...\n",
    "\n",
    "#### Tendréis que tener en cuenta también variables como número de baños Full Bath, cocinas Kitchen AbvGr o dormitorios Bedroom AbvGr. Generando por ejemplo (número de estas variables por encima de media)\n",
    "\n",
    "#### Finalmente precio de venta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
