# -*- coding: utf-8 -*-
"""examen2021v2_Beatriz_Cardaba.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ga2nqsxsGawpeYpcFVEaX9d_sP9clH4z

# Beatriz Cárdaba Rico

## TAREA FINAL: Datos de vivienda

<p> El negocio inmobiliario es uno de los motores de la economía de España, el carácter turístico de nuesta península hace que sean muchos los inversores extranjeros que se decidan por buscar una inversión de bajo riesgos en nuestras ciudades costeras o grandes urbes. 
Muchas de estas inversiones se hacen sobre grandes bolsas de inmuebles que deben ser analizados previamente para comprobar la rentabilidad del porfolio </p>

<!-- <p> En este caso vamos a trabajar con una tabla que contienen información de distintos inmuebles repartidos por una zona específica, sus carácterísticas y su precio </p>  -->

Todas las cuestiones se deben realizar sobre el conjunto de casos que representen viviendas ubicadas en zonas residenciales **(alta, media y baja densidad)**

**MUY IMPORTANTE:** En las otras prácticas he detectado colaboraciones involucrando varias personas y he sido flexible aunque a algunos os lo he mencionado en las correcciones, porque al final el trabajo de analista de datos es un trabajo colaborativo. Sin embargo, este trabajo es individual, así que cuidado con las colaboraciones.

Las variables de las que se compone el dataset son:

|NOMBRE VARIABLE|DESCRIPTOR|VALORES|
| --- | --- | --- |
|Order|Variable de identificación|1 a 2930|
|MS Zoning|Zona de ubicación de la vivienda|"A rural, C comercial, FV residencial flotante, I industrial, RH residencial alta densidad, RL residencial baja densidad, RM residencial media densidad"|
|Lot Frontage|Longitud de la fachada en pies||
|Lot Area|Superficie de la vivienda en pies cuadrados||
|Land Contour|Contorno del terreno circundante|"Lvl llano, Bnk Tipo bancal, HLS Ladera, Low Depresión"|
|Land Slope|Tipo de pendiente de la vivienda|" Gtl pendiente suave, Mod pendiente moderada, Sev fuerte pendiente"|
|Overall Qual|Grado de calidad de materiales y acabado de la vivienda|De 1 (Muy pobre) a 10 (Excelente)|
|Year Built|Año de construccion de la vivienda||
|Year Remod/Add|Año de última reforma de la vivienda||
|Mas Vnr Type|Tipo de revestimiento exterior|" BrkCmn Ladrillo normal, BrkFace Ladrillo visto, CBlock Bloque de cemento, None Ninguna, Stone Piedra "|
|Exter Qual|Calidad de revestimiento exterior|"Ex Excelente,Gd Bueno,TA Media,Fa Justo"|
|Bsmt Cond|Estado general del sótano|"Ex Excelente, Gd Bueno, TA Media, Fa Justo, Po Pobre,Ss sin sótano"|
|Total Bsmt SF|Superficie del sótano en pies cuadrados|
|Heating QC|Calidad de la calefacción|"Ex Excelente,Gd Bueno,TA Media,Fa Justo,Po Pobre"|
|Central Air|Aire acondicionado centralizado|"N No Y Sí"|
|Full Bath|Número de baños completo en planta||
|Half Bath|Número de aseos en planta||
|Bedroom AbvGr|Número de dormitorios en planta||
|Kitchen AbvGr|Número de cocinas en planta||
|Kitchen Qual|Calidad de cocinas|"Ex Excelente,Gd Bueno,TA Media,Fa Justo,Po Pobre"|
|TotRms AbvGrd|Número total de habitaciones excluidos los cuartos de baño||
|Garage Cars|Número de plazas de garaje||
|Garage Area|Superficie del garaje|||
|Garage Cond|Estado del garaje|"Ex Excelente,Gd Bueno,TA Media,Fa Justo,Po Pobre,Sg sin garaje"|
|Pool Area|Superficie de la piscina en pies cuadrados|
|Pool QC|Calidad de la piscina|"Ex Excelente,Gd Bueno,TA Media,Fa Justo,Sp no hay piscina"|
|Mo Sold|mes de venta||
|Yr Sold|año de venta||
|SalePrice|precio de venta en dólares||

Recomiendo al leer los datos, eliminar espacios de los nombres de las columnas, realiza un pequeño análisis inicial de los mismos. No olvides fijarte en los tipos de variables, que variables pueden tener tipos confundidos y corrige los. Sobre todo, trabaja con las fechas.

**NOTA:** Las tareas complementarias sirven para subir nota. El resto de preguntas valen igual y suman 10 puntos.

## Inicializar y cargar el contexto spark
"""

!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q https://www-us.apache.org/dist/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz
!tar xf spark-2.4.7-bin-hadoop2.7.tgz
!pip install -q findspark


import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-2.4.7-bin-hadoop2.7"

import findspark
findspark.init()
from pyspark import SparkContext

from pyspark import SparkContext
from pyspark.sql import *
from pyspark.sql import SQLContext, functions as F, Row
from pyspark.sql.types import *
from pyspark.sql.types import StringType, IntegerType

# Librería para dar formato a las salidas obtenidas
import pandas as pd
# Librería para aplicar expresiones regulares en el parseado
import re
# librería para parsear las fechas con formato date
from datetime import datetime
# Librería que permite descargar información de una url desde python
import urllib
from urllib.request import urlretrieve
# Librería para llamar a funciones relacionadas con el sistema operativo
import os

sc = SparkContext.getOrCreate()

sqlContext = SQLContext(sc)

"""## Importamos CSV"""

# se importan los datos en formato csv
data = "BDpracticafinalCSV.csv" 
raw_data = sc.textFile(data)

"""Parseo de datos: se observa que los datos están separados por ";". """

# separamos en columnas
dt_data= raw_data.map(lambda x: x.split(";"))

# visualizamos las dos primeras filas
dt_data.take(2)

"""## Cuántas viviendas distintas encontramos en el dataset? ¿Se repite alguna? Tiene sentido que haya duplicadas? ¿Qué podemos hacer con las duplicadas?

Para conocer cuantas viviendas hay y si están duplicadas, primero se van a realizar transformaciones en los datos para que la contabilización sea la correcta.

 - Se elimina la primera fila que es la cabecera, para que no se tenga en cuenta en los análisis
"""

# identificamos  la cabecera
header = dt_data.take(1)[0] 
header

# eliminamos la cabecera del rdd de los datos
dt_data_nh = dt_data \
    .filter(lambda line: line!=header)

"""## Filtramos las zonas residenciales (alta, media y baja densidad)
Se observa que estas zonas contienen diferentes nombres por lo que se decide eliminar aquellos que NO son de estas zonas.
Se eliminan de los datos las ventas de zonas A <- RURAL, C <- COMERCIAL, FV <- RESIDENCIAL FLOTANTE y I <- INDUSTRIAL.

Se seleccionan todas las columnas.

"""

dt_data_nh_filter = dt_data_nh.map(lambda x: (x[0], x[1], x[2], x[3], x[4], x[5], x[6], x[7], x[8], x[9], x[10], x[11], x[12], x[13], x[14], x[15], x[16], x[17], x[18], x[19], x[20], x[21], x[22],x[23],x[24],x[25],x[26],x[27],x[28])).\
                filter(lambda x: x[1] != ("A")).\
                filter(lambda x: x[1] != ("C") ).\
                filter(lambda x: x[1] != ("FV")).\
                filter(lambda x: x[1] != ("I") )

"""El rdd dt_data_nh_filter contiene todos los datos de las zonas seleccionadas sin cabecera.
De aquí en adelante todas las operaciones se realizarán sobre la muestra ya filtrada en estas zonas

A continuación conocemos cuantos registros contiene:
"""

# cuenta del número de filas
(dt_data_nh_filter.map(lambda line: line[0]).count())

"""Hay 2768 registros en los datos.

Se pasa el RDD a formato tabla SQL para poder realizar diferentes cálculos y operaciones.

Hay que tener en cuenta que se realiza con un rdd que no tiene cabecera.
"""

df_data = sqlContext.createDataFrame (dt_data_nh_filter  )

# visualizamos el dataframe SQL creado con el rdd
df_data.show(10)

"""Al no tener cabecera, es importante conocer que el orden es el mismo que el expuesto en metada. Siendo _1 la variable Order y _29 la variable de precio."""

df_data.createOrReplaceTempView("df_data") # necesario para operar con el df

"""Cuenta de valores únicos en el DF.
Para este cálculo se utiliza la primera columna "Order" que es un identificador único por vivienda:
"""

sqlContext.sql("""SELECT COUNT(DISTINCT _1) AS Nro_viviendas
                  FROM df_data """).show()

"""Hay 2768 viviendas únicas y 2762 registros. Esto nos indica que hay viviendas que se encunetran más de una vez en el registro.
Interpretando que tipo de base de datos y qué significa cada uno de los registros he decidido no eliminar los duplicados ya que una sola vivienda puede haber formado parte de dos transacciones diferentes en momentos diferentes, aunque sea en el mismo año. Esto puede aportar posible informacion acerca de la evolución de los precios de mercado.No existe en la metadata más infromación para descartar que una misma vivienda no pueda venderse y registrarse más de una vez y que esto sea una errata. Por lo tanto, se mantienen todos los registros.

## ¿Podrías decirme el total de inmuebles y el precio medio (Sale Price) de cada zona (MS Zoning)?

Mediante SPARK SQL se realiza la siguiente tabla que muestra el conteo de ventas por cada zona y su precio medio. 

Ha hanido que unificar las zonas de las viviendas ya que presentaban formas diferentes como rL y Rl.
"""

sqlContext.sql("""SELECT  count( _1) AS Nro_viviendas, AVG(_29) AS precio_medio,
                  CASE _2 WHEN 'Rl' THEN 'RL'
                          WHEN 'rL' THEN 'RL'
                          WHEN 'RL' THEN 'RL'
                          WHEN 'RH' THEN 'RH'
                          ELSE 'RM' END AS ZONA
                  FROM df_data 
                  GROUP BY ZONA """).show()

"""Como se ha explicado en el ejercicio anterior se decide tener en cuenta todos los registros ya que hay viviendas que pueden haberse vendido en más de una ocasión.

Por lo que esta tabla tabla muestra el total de inmuebles de la BBDD agrupados por zona e indicando su precio medio

## Media de Total Bsmt SF por cada década de construcción calculada a partir de Year Built. 
## ¿Cuál es la decada de construcción con viviendas mejor acondicionadas para el frío (Heating QC)?

El objetivo es agrupar cada transaccion por decadas calculada mediante el campo Year Buit. 

Despues, habrá que utilizar la clasificación Heating QC para identificar cual es la media de este ratio para cada década y así poder establecer un ranking por décadas en función de esta caraterística.

En primer lugar, para realizar la agrupación visualizamos el primer y el último año de construcción de las viviendas:
"""

sqlContext.sql("""SELECT  min( _8) , max(_8)
                  FROM df_data """).show()

"""La primera vivienda de la bbdd fue construida en 1872 y la última en 2010.

A continuación mediante SPARK SQL se realiza la agrupación por décadas, indicando en esta nueva columna el siglo y la década a la que pertence cada vivienda.

Además, para poder realizar un ratio acerca de la calidad de de su acondicionamiento al frío se crea otra nueva columna num_heatingQ, que identificará numericamente mediante la siguiente equivalencia: 

Heating QC	Calidad de la calefacción	"Ex Excelente = 5 ,Gd Bueno = 4,TA Media = 3,Fa Justo = 2 ,Po Pobre = 1".

Así las viviendas con mayor calificación numérica serán las mejor acondicionadas al frío.

Esta nueva identificación permite calcula cual es la media de la calidad del acondicionamiento de las viviendas según la década en la que fueron construidas. A mayor media de esta nueva variable en la década de construcción mejoe es el acondicionamiento
"""

from pyspark.sql.functions import*
df_data.createOrReplaceTempView("df_decade_quality")
df_decade_quality = sqlContext.sql(""" SELECT*, 
CASE
    WHEN _8 < 1880 THEN 'XIX_70s'
    WHEN _8<1890 THEN 'XIX_80s'
    WHEN _8<1900 THEN 'XIX_90s'
    WHEN _8<1910 THEN 'XX_00s'
    WHEN _8<1920 THEN 'XX_10s'
    WHEN _8<1930 THEN 'XX_20s'
    WHEN _8<1940 THEN 'XX_30s'
    WHEN _8<1950 THEN 'XX_40s'
    WHEN _8<1960 THEN 'XX_50s'
    WHEN _8<1970 THEN 'XX_60s'
    WHEN _8<1980 THEN 'XX_70s'
    WHEN _8<1990 THEN 'XX_80s'
    WHEN _8<2000 THEN 'XX_90s'
    WHEN _8<2010 THEN 'XXI_00s'
    ELSE 'XXI_10s'
END  AS decada,
CASE
    WHEN _14 = 'Ex' THEN 5
    WHEN _14 = 'Gd' THEN 4
    WHEN _14 = 'TA' THEN 3
    WHEN _14 = 'Fa' THEN 2
    ELSE 1
END  AS num_heatingQ
FROM df_data 
""").persist()

"""Se ha relaizado un persist para poder guardar las nuevas columnas creadas. Visualizamos los datos a continuación:"""

df_decade_quality.show(10)

df_decade_quality.createOrReplaceTempView("df_decade_quality")

"""__Media de tamaño de los sótanos en pies cuadrados por dácada de construcción:__

Una vez tenemos a que década ha pertenecido cada vivienda es posible calcula la media, en pies cuadrados, del tamaño de los sótanos. La variable Total Bsmt SF es la _13 de la tabla de datos.
"""

sqlContext.sql(""" SELECT decada , AVG(_13) AS media_sotano
 FROM df_decade_quality
 GROUP BY decada
 """).show()



"""__Década con las viviendas mejor acondcionadas al frío:__

Agrupamos los registros por décadas y calculamos la calidad media de sua condicionamiento al frío a traves de la columna numérica calculada.

La década con una media mayor será la mejor acondicionada al frío.
"""

sqlContext.sql(""" SELECT decada, AVG(num_quality) AS calidad_media
FROM df_decade_quality
GROUP BY decada
ORDER BY calidad_media DESC
""").show()

"""Se ha ordenado de mayor a menor calidad de acondicionamiento y el resultado muestra que la década con mviviendas mejores acondicionadas al frío es la década que comienza en el 2010. Por el contrario las viviendas peor acondicionadas son las construídas en la decada que comenzó en 1870.

## ¿Cuáles son las 10 viviendas que se vendieron por un precio más elevado por metro cuadrado en el año 2009?

Para calcular estas 10 viviendas se va a utilizar SPARK SQL. 
Hayq ue tener en cuenta que la información del tamaño de la vivienda viene dada en pies cuadrados. Para obtener los metros cuadrados se aplica : 1 pie cudrado = 0.092903 metros cuadrados.

Para conocer el precio de las viviendas por metro cuadrados hay que dividir el valor total de la vivienda entre los metros cuadrados:

Precio_Vivienda/(pies_cudrdados*0.092903)

El precio de la vivienda es la columna _29 y el tamaño en pies cuadrados es la columna _4 

Se añade un orden descente y nos quedamos con los 10 primeros registros:
"""

sqlContext.sql(""" SELECT _1 AS id_vivienda, _29/(_4*0.092903) AS precio_metros_cuadrados
FROM df_data
WHERE _28 =2009
ORDER BY precio_metros_cuadrados DESC
""").show(10)

"""La vivienda identificada con el número 936 fue la que se vendión por un precio más elevado por metros cuadrados, 1026 dolares por metro cuadrado

## Media anual por zonas del precio de venta y metros cuadrados.

El objetivo es identificar para cada año en el que estamos estudiando las ventas, cual es el precio medio de las viviendas en cada zona.
La bbdd de las ventas comienza en 2006 y finaliza en 2010.

Agrupamos las ventas por años y por zonas, y para cada una de estas combianciones estudiamos su precio medio y su tamaño medio en metros cuadrados.

Se ha procedido a realizar este cálculo mediante SPARK SQL. Hay que tener en cuenta que el año de venta es la columna _28, la zona la columna _2 (sobre la que realizamos la agrupación correcta anteriormente comentada), el precio la columna _29 y el tamaño en metros cuadrados los calculamos _4*0.092903
"""

sqlContext.sql(""" SELECT _28 AS YEAR, CASE _2 WHEN 'Rl' THEN 'RL'
                          WHEN 'rL' THEN 'RL'
                          WHEN 'RL' THEN 'RL'
                          WHEN 'RH' THEN 'RH'
                          ELSE 'RM' END AS ZONA, AVG(_29) AS precio_medio, AVG(_4*0.092903) AS metros_cuadrados_medios
FROM df_data
GROUP BY _28, ZONA
ORDER BY YEAR
""").show()

"""## ¿Podrías decirme el total de recaudación de las casas de revistimiento (Mas Vnr Type) de piedra con respecto a las de ladrillo? ¿Hay diferencia significativa?

El obtivo es identificar la suma total de los precios de las viviendas, analizando si influye que su revestimiento sea piedra o ladrillo.

En primer lugar bisualizamos todos los tipos de revestimiento y la suma de los precios de todas las ventas
"""

from pyspark.sql.functions import sum
sqlContext.sql("""  SELECT _10 AS material_revestimietno , COUNT(_1) AS num_viviendas,SUM(_29) AS precio_total
FROM df_data
GROUP BY _10
""").show()

"""
Hay que tener en cuenta que de ladrillo hay 875 ventas en total mientras que de piedra hay 230. Para poder hacer un cáculo en el que se puedan comparar mejor ambos recubrimientos calcularemos el precio medio para las de ladrillo y el precio medio para las de piedra.


Realizamos estos cáculos mediante SPARK SQL agrupando las casas por los carácteres contenidos en su tipo de recubriemietno,
Se observa que aquella que están revestidas con ladrillo tienen en común la parte Brk.
Y las de piedra Stone.

"""

from pyspark.sql.functions import sum
sqlContext.sql("""  SELECT AVG(_29) AS precio_medio_ladrillo
FROM df_data
WHERE _10 LIKE '%Brk%'
""").show()

from pyspark.sql.functions import sum
sqlContext.sql("""  SELECT AVG(_29) AS precio_medio_piedra
FROM df_data
WHERE _10 LIKE '%Stone%'
""").show()

"""Se observa que la recaudación media por vivienda es mayor para las recubiertas por piedra. Sin embargo, al haber más casas de ladrillo la recudación total es mayor la de las casas recubiertas de ladrillo

## ¿Cuánto son más caras las viviendas con 2 cocinas, con 2 o más plazas de garaje que las que tienen 1 cocina y 1 plaza de garaje? Comparar medias y cuartiles de ambos casos

El objetivo es conocer las estadísticas de las vivivendas con las caracterísiticas señaladas.


__Viendas con dos cocinas y dos o más plazas de garage.__

En primer lugar, se va a estudiar la distribución y los valores medios de los precios para viviedas con dos cocinas y dos o más plazas de garages.
Para ello se va trabajar con los rdd. Nos interesa filtrar aquellas viviendas con dos cocinas, el número de cocinas es la columna [18], y con dos o más garges, columna [21]. Además necesitamos conocer el precio de estas viviendas
"""

dt_2coci_2bath = dt_data_nh_filter.map(lambda x: (x[0],x[18],x[21],x[28])).\
                filter(lambda x: x[1] == '2').\
                filter(lambda x: x[2] >'1' )

dt_2coci_2bath.take(10) # visualizamos que ha cogido las características correctas

# indicamos el tipo de columnas con el que estamos trabajando ya que necesimaos hacer algunos cálculos

dt_2coci_2bath_clean = dt_2coci_2bath.map(lambda x: (str(x[0]),  float(x[1]), float(x[2]), float(x[3]) ))

# se crea el df para poder extraer las carácterisicas numéricas

df_2coci_2bath_clean = sqlContext.createDataFrame (dt_2coci_2bath_clean  )

"""Caracterísiticas numéricas: nos interesa las de la columna _4 ya que nos indica el precio"""

df_2coci_2bath_clean.summary().show()

"""Para viviendas con dos cocinas y dos o más plazas de garage el precio medio es 145124.28 dollars. En cuanto a los cuartiles, el cuartil 25% es 118964$, esto quiere decir que el 25% de las vivivendas de estas caracterísiticas su precio de venta fue menor que 118964. La mediana es 141000. Y el cuartil 75% es 159000, esto indica que el 75% de las viviendas de estas caracterísitcas  se vendieron por menos de 159000 dólares.

Comprobamos que la media es igual que calculandolo sobre el rdd
"""

dt_2coci_2bath_clean.map(lambda x: (x[3])).mean()

"""Realizamos el mismo porceso para:

__Viviendas con una cocina y una plaza de garage:__

Como se ha realiza anteriormente filtramos el rdd para que tenga las caraterísticas que se han indicado
"""

dt_1coci_1gara = dt_data_nh_filter.map(lambda x: (x[0],x[18],x[21],x[28])).\
                filter(lambda x: x[1] == '1').\
                filter(lambda x: x[2] =='1' )

"""Indicamos el tipo de columnas con el que estamos trabajando para poder realizar los cálculos sobre ellas:"""

dt_1coci_1gara_clean = dt_1coci_1gara.map(lambda x: (str(x[0]),  float(x[1]), float(x[2]), float(x[3]) ))

dt_1coci_1gara.take(10)

"""Se crea un data frame para poder realizar las medidas estadísitcas de las variables"""

df_1coci_1gara = sqlContext.createDataFrame (dt_1coci_1gara  )

"""Visualizamos las medidas estadísticas. La variable precio es la columna_4, con las otras dos columnas podemos comprobar que la muestra seleccionada cumple con los reqisitos"""

df_1coci_1gara.summary().show()

"""Para viviendas con una cocina y una plaza de garage el precio medio es 128122 dollars. En cuanto a los cuartiles, el cuartil 25% es 111500$, esto quiere decir que el 25% de las vivivendas de estas caracterísiticas su precio de venta fue menor que 111500. La mediana es 128000. Y el cuartil 75% es 142125, esto indica que el 75% de las viviendas de estas caracterísitcas  se vendieron por menos de 142125 dólares.

Comprobamos que la media es igual que calculandolo sobre el rdd
"""

dt_1coci_1gara_clean.map(lambda x: (x[3])).mean()

145124.27906976745 - 128121.9933774834 # diferencia en precios medios

141000 - 128000.0 # diferencia en las medianas

"""__COMPARACIÓN__

Analizando la media de los precios de las viviendas analizadas se observa que  la viviendas con dos cocionas y dos o más plazas de garage son en promedio 17002 dólares más caras. La mediana del primer tipo de viviendas es  13000 más que las del primer tipo. Estas diferencias se observan también en los valores de los cuartiles. Con esto, se puede concluir que al aumentar el número de cocinas de una a dos y de la pzadas de garage a más de una, supone un incremento en el precio medio de las viviendas.

## (COMPLEMENTARIA) Estudiar la relación entre el precio y el número de Garajes. (Recomiendo segmentar precio por cuartiles y estudiar el número de casos coincidentes)

Se desea conocer la posible correlación entre el precio y el número de garages. Ya que no se especifica la técnica a seguir se decide realizarlo de una manera simple que nos puede dar una idea de la exisitencia de correlaciones positivas o negativas para una fácil interpretación.
Se realiza mediante una fórmula de SPARK SQ, en la que la variable dependiente es el precio y la explicativa será el número de plazas de garages de la vivienda
"""

sqlContext.sql("""SELECT CORR(_29,_22) AS correlacion_precio_ngarages
                  FROM df_data """).show()

"""Se puede realizar una sencilla interpretación del resultado. Existe una correlación positiva entre el número de plazas de garage y el precio. Cuando las plazas de garage aumentan, el precio también aumenta.

## (COMPLEMENTARIA) Las 10 viviendas con mejores servicios y mejor precio.

### Pista:
#### Calcula las variables: 
- Número de servicios excelentes
- Número de servicios buenos
...

#### Tendréis que tener en cuenta también variables como número de baños Full Bath, cocinas Kitchen AbvGr o dormitorios Bedroom AbvGr. Generando por ejemplo (número de estas variables por encima de media)

#### Finalmente precio de venta

## (COMPLEMENTARIA) Importar los datos en formato EXCEL

A continuación se han importado los datos en excel para trabajar con ellos en un formato data frame en el que poder trabajar con SPARK
"""

data_excel = "BDpracticafinal2021.xlsx"

"""Importamos los datos nombrados y los visualizamos"""

pdf = pd.read_excel(data_excel)
pdf

"""Se observa que actualmente tienen forma de dataframe de python, por lo que es necesario realizar más transformaciones para porde leerlos con spark.

Es necesario importar el paquete que habilita la lectura de datos en excel:
"""

spark = SparkSession.builder \
.master("local") \
.appName("Word Count") \
.config("spark.jars.packages", "com.crealytics:spark-excel_2.11:0.12.2") \
.getOrCreate()

"""Definimos la estructura de los datos que vamos a importar. Hay que indicar que tipo de columna es cada una para que más tarde se pueda leer el excel correctamente y formar un data frame en el que se puedan realizar los cálculos en SPARK"""

from pyspark.sql.types import *

mySchema = StructType([ StructField("col1", StringType(), True)\
                       ,StructField("col2", StringType(), True)\
                       ,StructField("col3", FloatType(), True)\
                       ,StructField("col4", IntegerType(), True)\
                       ,StructField("col5", StringType(), True)\
                       ,StructField("col6", StringType(), True)\
                       ,StructField("col7", IntegerType(), True)\
                       ,StructField("col8", IntegerType(), True)\
                       ,StructField("col9", IntegerType(), True)\
                       ,StructField("col10", StringType(), True)\
                       ,StructField("col11", StringType(), True)\
                       ,StructField("col12", StringType(), True)\
                       ,StructField("col13", IntegerType(), True)\
                       ,StructField("col14", StringType(), True)\
                       ,StructField("col15", StringType(), True)\
                       ,StructField("col16", IntegerType(), True)\
                       ,StructField("col17", IntegerType(), True)\
                       ,StructField("col18", IntegerType(), True)\
                       ,StructField("col19", IntegerType(), True)\
                       ,StructField("col20", StringType(), True)\
                       ,StructField("col21", IntegerType(), True)\
                       ,StructField("col22", FloatType(), True)\
                       ,StructField("col23", FloatType(), True)\
                       ,StructField("col24", StringType(), True)\
                       ,StructField("col25", IntegerType(), True)\
                       ,StructField("col26", StringType(), True)\
                       ,StructField("col27", IntegerType(), True)\
                       ,StructField("col28", IntegerType(), True)\
                       ,StructField("col29", IntegerType(), True)
                       ])

"""Aplicamos al data frame de python que hemos creado el formato de datos que hemos indicado en el esquema"""

df = spark.createDataFrame(pdf,schema=mySchema)

"""Se muestra el data frame creado en SPARK"""

df.show()