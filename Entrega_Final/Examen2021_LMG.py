# -*- coding: utf-8 -*-
"""examen2021v2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LPA8m0b8v7Xtj-fBYZCFku0Z1pYJNi0L

*Laura Martínez González De Aledo*

# TAREA FINAL: Datos de vivienda

<p> El negocio inmobiliario es uno de los motores de la economía de España, el carácter turístico de nuesta península hace que sean muchos los inversores extranjeros que se decidan por buscar una inversión de bajo riesgos en nuestras ciudades costeras o grandes urbes. 
Muchas de estas inversiones se hacen sobre grandes bolsas de inmuebles que deben ser analizados previamente para comprobar la rentabilidad del porfolio </p>

<!-- <p> En este caso vamos a trabajar con una tabla que contienen información de distintos inmuebles repartidos por una zona específica, sus carácterísticas y su precio </p>  -->

Todas las cuestiones se deben realizar sobre el conjunto de casos que representen viviendas ubicadas en zonas residenciales **(alta, media y baja densidad)**

Las variables de las que se compone el dataset son:

|NOMBRE VARIABLE|DESCRIPTOR|VALORES|
| --- | --- | --- |
|Order|Variable de identificación|1 a 2930|
|MS Zoning|Zona de ubicación de la vivienda|"A rural, C comercial, FV residencial flotante, I industrial, RH residencial alta densidad, RL residencial baja densidad, RM residencial media densidad"|
|Lot Frontage|Longitud de la fachada en pies||
|Lot Area|Superficie de la vivienda en pies cuadrados||
|Land Contour|Contorno del terreno circundante|"Lvl llano, Bnk Tipo bancal, HLS Ladera, Low Depresión"|
|Land Slope|Tipo de pendiente de la vivienda|" Gtl pendiente suave, Mod pendiente moderada, Sev fuerte pendiente"|
|Overall Qual|Grado de calidad de materiales y acabado de la vivienda|De 1 (Muy pobre) a 10 (Excelente)|
|Year Built|Año de construccion de la vivienda||
|Year Remod/Add|Año de última reforma de la vivienda||
|Mas Vnr Type|Tipo de revestimiento exterior|" BrkCmn Ladrillo normal, BrkFace Ladrillo visto, CBlock Bloque de cemento, None Ninguna, Stone Piedra "|
|Exter Qual|Calidad de revestimiento exterior|"Ex Excelente,Gd Bueno,TA Media,Fa Justo"|
|Bsmt Cond|Estado general del sótano|"Ex Excelente, Gd Bueno, TA Media, Fa Justo, Po Pobre,Ss sin sótano"|
|Total Bsmt SF|Superficie del sótano en pies cuadrados|
|Heating QC|Calidad de la calefacción|"Ex Excelente,Gd Bueno,TA Media,Fa Justo,Po Pobre"|
|Central Air|Aire acondicionado centralizado|"N No Y Sí"|
|Full Bath|Número de baños completo en planta||
|Half Bath|Número de aseos en planta||
|Bedroom AbvGr|Número de dormitorios en planta||
|Kitchen AbvGr|Número de cocinas en planta||
|Kitchen Qual|Calidad de cocinas|"Ex Excelente,Gd Bueno,TA Media,Fa Justo,Po Pobre"|
|TotRms AbvGrd|Número total de habitaciones excluidos los cuartos de baño||
|Garage Cars|Número de plazas de garaje||
|Garage Area|Superficie del garaje|||
|Garage Cond|Estado del garaje|"Ex Excelente,Gd Bueno,TA Media,Fa Justo,Po Pobre,Sg sin garaje"|
|Pool Area|Superficie de la piscina en pies cuadrados|
|Pool QC|Calidad de la piscina|"Ex Excelente,Gd Bueno,TA Media,Fa Justo,Sp no hay piscina"|
|Mo Sold|mes de venta||
|Yr Sold|año de venta||
|SalePrice|precio de venta en dólares||

Recomiendo al leer los datos, eliminar espacios de los nombres de las columnas, realiza un pequeño análisis inicial de los mismos. No olvides fijarte en los tipos de variables, que variables pueden tener tipos confundidos y corrige los. Sobre todo, trabaja con las fechas.

**NOTA:** Las tareas complementarias sirven para subir nota. El resto de preguntas valen igual y suman 10 puntos.

## Inicializar y cargar el contexto spark

SparkContext es lo primero que se crea en un programa con Spark, es el que nos da el accesso al clúster. Permite crear RDDs a partir de ficheros de datos.
"""

!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q https://www-us.apache.org/dist/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz
!tar xf spark-2.4.7-bin-hadoop2.7.tgz 
!pip install -q findspark 

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64" 
os.environ["SPARK_HOME"] = "/content/spark-2.4.7-bin-hadoop2.7"

import findspark 
findspark.init()
from pyspark import SparkContext
sc = SparkContext()
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[*]").getOrCreate()

"""* Carga de datos:"""

data_file = "./BDpracticafinalCSV.csv" 
datos = sc.textFile(data_file)

"""Hemos creado nuestro objeto RDD. Un RDD es la unidad básica de computación en Spark. Es conceptualmente similar a una lista tradicional y contiene cualquier tipo de elemento (int, float, tuplas, listas...). Lo creamos a partir de la base de datos "BDpracticafinalCSV".

* Comprobamos que se ha cargador correctamente y vemos cuál es el separador:
"""

datos.take(3)

"""* Numero de registros:"""

datos.count()

"""* Dejo el fichero ya dividido por los ' ; '"""

datos_split = datos.map(lambda x: x.split(";"))

datos_split.take(2)

"""* El enunciado pide que usemos unicamente las zonas de alta, media y baja densidad:"""

datos_filtrados = datos_split.filter(lambda x: (x[1] == 'RL', x[1] == 'Rl', x[1] == 'rL', x[1] == 'RH', x[1] == 'RM'))

"""* Quito la cabecera"""

header = datos_filtrados.first()
datos_split_wh = datos_filtrados.filter(lambda x: x != header)

"""## 1. Cuántas viviendas distintas encontramos en el dataset? ¿Se repite alguna? Tiene sentido que haya duplicadas? ¿Qué podemos hacer con las duplicadas?

* Viviendas distintas.
Cogemos como referencia 'Order' por ser la variable de identificación:
"""

datos_split_wh.map(lambda x: x[0]).distinct().count()

"""Vemos que si comparamos el total de viviendas con lo anterior calculado, salen 6 menos (sin tener en cuenta la cabecera). Por lo que podríamos afirmar que sí que hay duplicados y la manera de tratarlos sería estableciendo tantas expresiones regulares como casos haya en el fichero. O mediante "drop_duplicates()" si trataramos con SparkDataFrame.

## 2. ¿Podrías decirme el total de inmuebles y el precio medio (Sale Price) de cada zona (MS Zoning)?

A continuación, hemos calculado con un *.count()* el total de viviendas en cada zona y posteriormente habría que calcular el precio medio de esas viviendas:
"""

from pyspark.mllib.stat import Statistics #Archivo Cunef7

"""* Zona RL: (incluimos 'rL', 'Rl')"""

zonaRL = datos_split_wh.\
         filter(lambda x: (x[1] == 'RL', x[1] == 'Rl', x[1] == 'rL')).\
         map(lambda x: (x[0],x[28])).\
         reduceByKey(lambda a,b:a + b)
zonaRL.count()

"""*  Zona RH:"""

zonaRH = datos_split_wh.\
         filter(lambda x: x[1] == 'RH').\
         map(lambda x: (x[0],x[28])).\
         reduceByKey(lambda a,b:a + b)
zonaRH.count()

"""* Zona RM:"""

zonaRM = datos_split_wh.\
         filter(lambda x: x[1] == 'RM').\
         map(lambda x: (x[0],x[28])).\
         reduceByKey(lambda a,b:a + b)
zonaRM.count()

"""## 3. Media de Total Bsmt SF por cada década de construcción calculada a partir de Year Built. 
## ¿Cuál es la decada de construcción con viviendas mejor acondicionadas para el frío (Heating QC)?
"""

from datetime import datetime
dateparse = lambda x: datetime.fromtimestamp(float(x))

"""* Creamos una función para sacar las décadas y la aplicamos a la columna de 'Year Built':"""

def decadas(x):
    return x[7] + "0s"

decadas("2000-2010")

# fechas_parse = datos_split_wh.map(lambda x: (decadas(x[0],x[7],x[12],x[13]))).reduceByKey(lambda x,y : x+y).sortBy(lambda x: -x[7])

"""* Otra forma de hacerlo es filtrando por 'Excelente' en la columna que indica el estado de la vivienda para el frío:"""

datos_split_wh.\
filter(lambda x: x[13] == 'Ex').\
map(lambda x: (x[0],x[7],x[12],x[13])).\
takeOrdered(10, lambda x: -float(x[1])).\
collect()

"""Una vez filtrado hemos ordenador por año debido que cuanto más nueva sea la vivienda, mejor acondicionada estará.

## 4. ¿Cuáles son las 10 viviendas que se vendieron por un precio más elevado por metro cuadrado en el año 2009?

* En primer lugar habría que pasar la superficie del area medida en pies a metros cuadrados multiplicando por 0.929. A continuación, filtramos la columna año de construcción por 2009 y ordenamos en funcion de la columna del precio de venta.
"""

datos_split_wh.filter(lambda x: x[7] == '2009').map(lambda x: (x[0],x[2],x[7],x[-1])).takeOrdered(10, lambda x: -float(x[-1]))

"""## 5. Media anual por zonas del precio de venta y metros cuadrados.

Habría que cambiar la medida de la variable superficie de pies a metros cuadrados. Y posteriormente, calcular la media con *.mean()*
"""

datos_split_wh.\
filter(lambda x: (x[1],x[7])).\
map(lambda x: (x[3],x[1],x[7],x[-1])).\
collect()

"""## 6. ¿Podrías decirme el total de recaudación de las casas de revistimiento (Mas Vnr Type) de piedra con respecto a las de ladrillo? ¿Hay diferencia significativa?

* Una forma de hacerlo sería hacerlo con esta librería:
"""

from pyspark.sql.functions import sum

"""Mediante Excel, filtrando por *BrkFace* y *Stone* obtenemos que hay un total de recaudación de 186.003.261 y 65.196.277, respectivamente.


Como ha sido posible calcularlo con Spark debido que la función de acción *.sum()* ha seguido dando error después de muchos intentos. La alumna ha querido demostrar su interés y pocos, pero asentados, conocimientos en esta herramienta, calculando de qué material está hecha la vivienda más cara de nuestra base de datos:
"""

datos_split_wh.\
        filter(lambda x: x[9] == 'Stone').\
        map(lambda x: (x[-1],x[9])).\
        takeOrdered(3, lambda x: -float(x[0]))

datos_split_wh.\
        filter(lambda x: x[9] == 'BrkFace').\
        map(lambda x: (x[-1],x[9])).\
        takeOrdered(3, lambda x: -float(x[0]))

"""Hemos aplicado primero un filter para distinguir entre las viviendas construidas en piedra o  las que están constridas en ladrillo. Posteriormente, hemos ordenado en función del precio de cada una (que una vez filtrado, se convierte en la columna 0) y hemos comprobado que las casas de ladrillo (con un máximo de 755000) son significativamente más caras que las de piedra (con un máximo de 615000).

## 7. ¿Cuánto son más caras las viviendas con 2 cocinas, con 2 o más plazas de garaje que las que tienen 1 cocina y 1 plaza de garaje? Comparar medias y cuartiles de ambos casos
"""

x = datos_split_wh.map(lambda x: (int(x[-1]))).collect()

x.sort()

"""Calculo los cuartiles:"""

x[int(len(x)*.25)]  # 25%

x[int(len(x)*.50)]  # 50%

x[int(len(x)*.75)]  # 75%

"""Defino una función para aplicar luego mediante un map, me clasifica los puntos en los cuartiles."""

def cuartiles(puntos):
    if puntos <= 129500:
        if puntos <= 160250:
            return (1)
        else:
            return (2)
    else:
        if puntos <= 213750:
            return (3)
        else:
            return (4)

"""Aplico el map con la función cuartiles, genero también un contador, de esta forma tendremos un rdd con tuplas que tendrán como key o label el cuartil correspondiente y la frecuencia como valor.

Viviendas con 2 cocinas, con 2 o más plazas de garaje
"""

datos_split_wh.\
          filter(lambda x: (int(x[18] == 2),(x[21] == 3))).\
          map(lambda x: (cuartiles(int(x[-1])),1)).\
          reduceByKey(lambda y,z: y + z).\
          collect()

"""Viviendas con1 cocina y 1 plaza de garaje"""

datos_split_wh.\
          filter(lambda x: (int(x[18] == 1),(x[21] == 1))).\
          map(lambda x: (cuartiles(int(x[-1])),1)).\
          reduceByKey(lambda y,z: y + z).\
          collect()

"""Aplicando los filters en función de si la casa tenía 1 o 2 cocinas o con 1 ó 2 y más plazas de garajes, en nuestra función de cuartiles, podemos afirmar que no existen una diferencia en el precio si comparamos los dos casos.

## 8. (COMPLEMENTARIA) Estudiar la relación entre el precio y el número de Garajes. (Recomiendo segmentar precio por cuartiles y estudiar el número de casos coincidentes)

* Aplicamos la función *cuartiles* creada en el ejercicio anterior:
"""

datos_split_wh.\
        filter(lambda x: ((x[21],x[-1]))).\
        map(lambda x: (cuartiles(int(x[-1]), ),1)).\
        reduceByKey(lambda y,z: y + z).\
        takeOrdered(10, lambda x: -float(x[-1]))

"""
* Otra forma de hacerlo que me resulta interesante sería filtrar entre las viviendas que tienen solo un garaje y las que tienen 3 y comparar el precio entre ellas:"""

datos_split_wh.\
        filter(lambda x: x[21] == 1).\
        map(lambda x: (x[21],x[-1])).\
        takeOrdered(5, lambda x: -float(x[1]))

datos_split_wh.\
        filter(lambda x: x[21] == 3).\
        map(lambda x: (x[21],x[-1])).\
        takeOrdered(5, lambda x: -float(x[1]))

"""* Otra forma de hacerlo sería mediante una correlación:"""

from pyspark.mllib.stat import Statistics  # Archivo Cunef7

datos_split_wh.select.stat.corr(x[21],x[-1])

"""## 9. (COMPLEMENTARIA) Las 10 viviendas con mejores servicios y mejor precio."""

datos_split_wh.\
filter(lambda x: ((x[7] == 2010), (x[13] == 'Ex'), (x[14] == 3), (x[17] == 4), (x[18] == 2))).\
map(lambda x: (x[0],x[1],x[-1])).\
takeOrdered(10, lambda x: -float(x[-1]))

"""Hemos escogido las casas que tienen mejores condiciones filtrando porque sean construidas en el año 2010, esté acondicionada excelentemente, tenga dos cocinas, tres dormitorios y dos baños, y hemos comprobado que entre ellas se encuentra la más cara de ladrillo que hemos calculado en el apartado 6 y que todas pertenecen a la zona RL.

### Pista:
#### Calcula las variables: 
- Número de servicios excelentes
- Número de servicios buenos
...

#### Tendréis que tener en cuenta también variables como número de baños Full Bath, cocinas Kitchen AbvGr o dormitorios Bedroom AbvGr. Generando por ejemplo (número de estas variables por encima de media)

#### Finalmente precio de venta
"""

sc.stop()